# Import package
import tweepy
from textblob import TextBlob
import jsonpickle
import json
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from googletrans import Translator
import wordcloud

#---------------------------------------------------------------------------------------------------------------------------
# API Authentication

# Store OAuth authentication credentials in relevant variables
access_token = ""
access_token_secret = ""
consumer_key = ""
consumer_secret = ""

# Pass OAuth details to tweepy's OAuth handler
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token,access_token_secret)

# cria um objeto api
api = tweepy.API(auth)

#---------------------------------------------------------------------------------------------------------------------------
#Reading and analyzing tweets from an id
analyser = SentimentIntensityAnalyzer()

#user_id  = 'BarackObama'
user_id  = 'realDonaldTrump'
#user_id  = 'jairbolsonaro'
count = 200

tweets = api.user_timeline(user_id, count = 5, tweet_mode='extended')

for t in tweets:
    print(t.full_text)
    print()

###---------------------------------------------------------------------------------------------------------------------------
## sentiment analyzer with translator
translator = Translator()

def sentiment_analyzer_scores(text, engl=True):
    #translator
    if engl:
        trans = text
    else:
        trans = translator.translate(text).text
    #score analyser
    score = analyser.polarity_scores(trans)
    lb = score['compound']
    if lb >= 0.05:
        return 1
    elif (lb > -0.05) and (lb < 0.05):
        return 0
    else:
        return -1

#####---------------------------------------------------------------------------------------------------------------------------
## make tweets to list
def list_tweets(user_id, count):
    tweets = api.user_timeline("@" + user_id, count=count, tweet_mode='extended')
    tw = [] # lista de tweets inicialmente vazia
    for t in tweets:
        tw.append(t.full_text)
    return tw

tw = list_tweets(user_id, count)

## remove patterns
def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)        
    return input_txt

## clean tweets
def clean_tweets(lst):
    # remove twitter Return handles (RT @xxx:)
    lst = np.vectorize(remove_pattern)(lst, "RT @[\w]*:")
    # remove twitter handles (@xxx)
    lst = np.vectorize(remove_pattern)(lst, "@[\w]*")
    # remove URL links (httpxxx)
    lst = np.vectorize(remove_pattern)(lst, "https?://[A-Za-z0-9./]*")
    # remove special car
    lst = np.core.defchararray.replace(lst, "á", "a")
    lst = np.core.defchararray.replace(lst, "é", "e")
    lst = np.core.defchararray.replace(lst, "í", "i")
    lst = np.core.defchararray.replace(lst, "ó", "o")
    lst = np.core.defchararray.replace(lst, "ú", "u")
    lst = np.core.defchararray.replace(lst, "Á", "A")
    lst = np.core.defchararray.replace(lst, "É", "E")
    lst = np.core.defchararray.replace(lst, "Í", "I")
    lst = np.core.defchararray.replace(lst, "Ó", "O")
    lst = np.core.defchararray.replace(lst, "Ú", "U")
    lst = np.core.defchararray.replace(lst, "â", "a")
    lst = np.core.defchararray.replace(lst, "ê", "e")
    lst = np.core.defchararray.replace(lst, "î", "i")
    lst = np.core.defchararray.replace(lst, "ô", "o")
    lst = np.core.defchararray.replace(lst, "Â", "A")
    lst = np.core.defchararray.replace(lst, "Ê", "E")
    lst = np.core.defchararray.replace(lst, "Î", "I")
    lst = np.core.defchararray.replace(lst, "Ô", "O")
    lst = np.core.defchararray.replace(lst, "ã", "a")
    lst = np.core.defchararray.replace(lst, "õ", "o")
    lst = np.core.defchararray.replace(lst, "Ã", "A")
    lst = np.core.defchararray.replace(lst, "Õ", "O")
    lst = np.core.defchararray.replace(lst, "ç", "c")
    lst = np.core.defchararray.replace(lst, "Ç", "C")
    # remove special characters, numbers, punctuations (except for #)
    lst = np.core.defchararray.replace(lst, "[^a-zA-Z#]", " ")
    return lst

tw = clean_tweets(tw)

print(tw[2])

print(sentiment_analyzer_scores(tw[2]))


#---------------------------------------------------------------------------------------------------------------------------
## tweets analyzer
def anl_tweets(lst, title='Tweets Sentiment', engl=True ):
    sents = []
    for tw in lst:
        try:
            st = sentiment_analyzer_scores(tw, engl)
            sents.append(st)
        except:
            sents.append(0)
    ax = sns.distplot(sents, kde=False, bins=3)
    ax.set(xlabel='Negative                Neutral                 Positive',
           ylabel='#Tweets',
           title="Tweets of @"+title)
    return sents

tw_sent = anl_tweets(tw, user_id)

print(tw_sent)

plt.show()


#---------------------------------------------------------------------------------------------------------------------------

# def word_cloud(wd_list):
#     stopwords = set(STOPWORDS)
#     all_words = ' '.join([text for text in wd_list])
#     wordcloud = WordCloud(
#         background_color='white',
#         stopwords=stopwords,
#         width=1600,
#         height=800,
#         random_state=21,
#         colormap='jet',
#         max_words=50,
#         max_font_size=200).generate(all_words)
#     plt.figure(figsize=(12, 10))
#     plt.axis('off')
#     plt.imshow(wordcloud, interpolation="bilinear");

# word_cloud(tw)
