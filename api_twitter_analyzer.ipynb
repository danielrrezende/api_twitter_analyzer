{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# Import package\nimport tweepy\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport csv\nfrom textblob import TextBlob\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom googletrans import Translator","execution_count":17,"outputs":[{"output_type":"stream","text":"[]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# API Authentication\n\nStore OAuth authentication credentials in relevant variables"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#---------------------------------------------------------------------------------------------------------------------------\n# API Authentication\n\n# Store OAuth authentication credentials in relevant variables\naccess_token = \"1110590844120248321-5V8MMFemfsdDbpnK5c3KsuQHLyrUR2\"\naccess_token_secret = \"OpNwpIoSh9LmxxStepn0wNGCDXrLT1KG0b3zJxCx6weer\"\nconsumer_key = \"gyK0Qlky1MZSPh1zrU5bLURIp\"\nconsumer_secret = \"IzlJiiumWK0xQey7aRVHRe34Awp239ExxjDPAiGZbRl1jgli6k\"\n\n# Pass OAuth details to tweepy's OAuth handler\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token,access_token_secret)\n\n# cria um objeto api\napi = tweepy.API(auth)\nsents = []","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# create an object called 'customStreamListener'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create an object called 'customStreamListener'\ndef twitter_stream_listener(filter_track):\n    \n    # class foi stream listener\n    class classStreamListener(tweepy.StreamListener):\n        \n        def on_status(self, status):\n\n            # translator ---------------------------------------------------------------------------------------------------\n            def translate_text(status_text):\n                analysis = TextBlob(status_text)\n                lang_detected = analysis.detect_language()\n               \n                if analysis.detect_language() != 'en':\n                    if analysis.detect_language() == 'nl': # got some error with nl language\n                        trans = analysis\n                    else:\n                        #trans = TextBlob(str(analysis.translate(from_lang=lang_detected, to='en')))\n                        trans = Translator().translate(status_text).text\n                else:\n                    trans = analysis\n                    \n                return trans\n            \n            \n            # sentiment analyzer -------------------------------------------------------------------------------------------\n            def sentiment_analyzer_scores(tx_trans):\n                #score analyser\n                score = SentimentIntensityAnalyzer().polarity_scores(tx_trans)\n                lb = score['compound']\n                if lb >= 0.05:\n                    return 1\n                elif (lb > -0.05) and (lb < 0.05):\n                    return 0\n                else:\n                    return -1\n            \n            tx_trans = translate_text(status.text.encode('latin1', errors='ignore').decode('latin1', errors='ignore'))\n            st = sentiment_analyzer_scores(tx_trans)\n\n##          print('lang_detected =', lang_detected) # DEBUG\n##          print(lb) # DEBUG       \n##          print(tx_trans) # DEBUG mostra o valor real do sentimento antes do filtro # DEBUG\n##          print('') # DEBUG\n##          print('Mensagem', status.text.encode('latin1', errors='ignore').decode('latin1', errors='ignore')) # DEBUG\n##          print('O tipo é', type(status.text)) # DEBUG\n##          print('comeco tweepy') # DEBUG\n##          print(status.text.encode('latin1', errors='ignore').decode('latin1', errors='ignore')) # DEBUG\n##          print('fim tweepy') # DEBUG\n##          print('') # DEBUG\n\n            \n            #list_tweets --------------------------------------------------------------------------------------------------\n            def list_tweets(status_text):\n                tw = [] # lista de tweets inicialmente vazia\n                tw.append(status_text)\n                return tw\n\n            tw = list_tweets(status.text.encode('latin1', errors='ignore').decode('latin1', errors='ignore'))\n            \n##          print('O tipo é', type(tw)) # DEBUG\n##          print('comeco lista') # DEBUG\n##          print(tw) # DEBUG\n##          print('fim lista') # DEBUG\n##          print('') # DEBUG\n\n            \n            ## remove patterns ---------------------------------------------------------------------------------------------\n            def remove_pattern(input_txt, pattern):\n                r = re.findall(pattern, input_txt)\n                for i in r:\n                    input_txt = re.sub(i, '', input_txt)        \n                return input_txt\n\n            \n            ## clean tweets ------------------------------------------------------------------------------------------------\n            def clean_tweets(lst):\n                # remove twitter Return handles (RT @xxx:)\n                lst = np.vectorize(remove_pattern)(lst, \"RT @[\\w]*:\")\n                # remove twitter handles (@xxx)\n                #lst = np.vectorize(remove_pattern)(lst, \"@[\\w]*\")\n                lst = np.core.defchararray.replace(lst, \"@\", \" \")\n                # remove URL links (httpxxx)\n                lst = np.vectorize(remove_pattern)(lst, \"https?://[A-Za-z0-9./]*\")\n                # remove special characters, numbers, punctuations (except for #)\n                lst = np.core.defchararray.replace(lst, \"[^a-zA-Z#]\", \" \")\n                return lst\n\n            tw = clean_tweets(tw)\n            \n##          print('O tipo é', type(tw)) # DEBUG\n##          print('comeco array') # DEBUG\n##          print(tw) # DEBUG\n##          print('fim array') # DEBUG\n##          print('') # DEBUG\n\n            \n            ## tweets analyzer and graph plot --------------------------------------------------------------------------------\n            def anl_tweets(tx_trans, title='Tweets Sentiment'):\n                st = sentiment_analyzer_scores(tx_trans)\n                sents.append(st)\n                ax = sns.distplot(sents, kde=False, bins=5)\n                ax.set(xlabel='Negative                            Neutral                             Positive', ylabel='#Tweets', title=\"Tweets of @\"+title)\n                plt.show(block=False)    \n                return sents\n\n            tw_sent = anl_tweets(tx_trans)\n            \n##          print('tw', tw) # DEBUG\n##          print('st', st) # DEBUG\n##          print('O tipo de st é', type(st)) # DEBUG\n##          print('sents', sents) # DEBUG\n##          print('O tipo de sents é', type(sents))\n##          print('tw_sent', tw_sent) # DEBUG\n##          print('O tipo de tw_sent é', type(tw_sent)) # DEBUG\n\n#---------------------------------------------------------------------------------------------------------------------------\n# write data in file\n\n            print('#---------------------------------------------------------------------------------------------------------------------------')\n            print('')\n            print(status.author.screen_name, status.created_at, tw)\n            # Writing status data\n            with open('OutputStreaming.txt', 'a') as f:\n                writer = csv.writer(f)\n##                writer.writerow([status.author.screen_name, status.created_at, tw, st])\n                if st == 1:\n                    print('O sentimento da mensagem é POSITIVE')\n                    writer.writerow([status.author.screen_name, status.created_at, tw, 'POSITIVE'])\n                elif st == 0:\n                    print('O sentimento da mensagem é NEUTRAL')\n                    writer.writerow([status.author.screen_name, status.created_at, tw, 'NEUTRAL'])\n                else:\n                    print('O sentimento da mensagem é NEGATIVE')\n                    writer.writerow([status.author.screen_name, status.created_at, tw, 'NEGATIVE'])\n            print('')\n            \n#---------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------\n# error            \n        def on_error(self, status_code):\n            if status_code == 420:\n                print('Encountered error code 420. Disconnecting the stream')\n            # returning False in on_data disconnects the stream\n                return False\n            else:\n                print('Encountered error with status code: {}'.format(status_code))\n                return True  # Don't kill the stream\n\n#---------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------\n# timeout\n        def on_timeout(self):\n            print(sys.stderr, 'Timeout...')\n            return True # Don't kill the stream\n\n#---------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------\n# Writing csv titles\n    with open('OutputStreaming.txt', 'w') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Author', 'Date', 'Text', 'Sentimental'])\n\n#---------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------\n    streamingAPI = tweepy.streaming.Stream(auth, classStreamListener())\n    \n    streamingAPI.filter(track=filter_track)\n\n    f.close()\n\n#---------------------------------------------------------------------------------------------------------------------------\n\n#filter_track = ['realDonaldTrump','wall']\n#filter_track = ['jairbolsonaro','previdencia']","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# filter_track"},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter_track = ['realDonaldTrump','wall']\n#filter_track = ['jairbolsonaro','previdencia']\n\nprint('-------------------------------------------------------------')\nprint('API TWITTER Portuguese ANALYZER')\nprint('-------------------------------------------------------------')\nprint('Please Type the filter that you want to analyze in Twitter')\nprint('e.g. \"jairbolsonaro\" and \"previdencia\"')\nprint('To FINISH, press ctrl+c and visualize the sentimental graph')\n\nfilter_track1 = input('First filter: ')\nfilter_track2 = input('Second filter: ')\nfilter_track = [filter_track1, filter_track2]\ntwitter_stream_listener(filter_track)","execution_count":21,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}